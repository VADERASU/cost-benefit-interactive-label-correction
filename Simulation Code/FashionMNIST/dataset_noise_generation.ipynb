{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GxlYlGnB4j7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.image as mpimg\n",
        "import io\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from noise_generate import get_noisy_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxDI_mXsCGa_",
        "outputId": "65dbd1f5-a239-48c5-e86f-9533398c1176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "input_shape = (28, 28, 1)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaT2ToNBf-GT"
      },
      "outputs": [],
      "source": [
        "noise_ratio_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.45, 0.5, 0.55, 0.6, 0.65]\n",
        "noise_type_list = ['uniform', 'class-dependent', 'locally-concentrated']  # corresponding to three noise types: NCAR, NAR, NNAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.isdir('fashionMNIST_sampled_data'):\n",
        "    os.mkdir('fashionMNIST_sampled_data')\n",
        "if not os.path.isdir('fashionMNIST_logits_and_preds'):\n",
        "    os.mkdir('fashionMNIST_logits_and_preds')\n",
        "if not os.path.isdir('fashionMNIST_noisy_data'):\n",
        "    os.mkdir('fashionMNIST_noisy_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CE64RCVpjxW"
      },
      "outputs": [],
      "source": [
        "def sample_class(x_train, y_train, x_test, y_test, train_idxes, test_idxes, classes):\n",
        "    # sampling instances by sampled classes\n",
        "    x_train_sampled_class = list()\n",
        "    y_train_sampled_class = list()\n",
        "    x_test_sampled_class = list()\n",
        "    y_test_sampled_class = list()\n",
        "    train_sampled_class_idxes = list()\n",
        "    test_sampled_class_idxes = list()\n",
        "\n",
        "    # For training set\n",
        "    for i in range(0, len(train_idxes)):\n",
        "      if y_train[i] in classes:\n",
        "        x_train_sampled_class.append(x_train[i])\n",
        "        y_train_sampled_class.append(y_train[i])\n",
        "        train_sampled_class_idxes.append(train_idxes[i])\n",
        "    # For testing set\n",
        "    for i in range(0, len(test_idxes)):\n",
        "      if y_test[i] in classes:\n",
        "        x_test_sampled_class.append(x_test[i])\n",
        "        y_test_sampled_class.append(y_test[i])\n",
        "        test_sampled_class_idxes.append(test_idxes[i])\n",
        "\n",
        "    return np.array(x_train_sampled_class), np.array(y_train_sampled_class), np.array(x_test_sampled_class), np.array(y_test_sampled_class), train_sampled_class_idxes, test_sampled_class_idxes\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh3-jZC_dY6F"
      },
      "outputs": [],
      "source": [
        "def sample_dataset(x_train, y_train, x_test, y_test, frac):\n",
        "    # get idxes list of each class in training set and testing set\n",
        "    class_idx_dict_train = dict()\n",
        "    for idx in range(0, y_train.shape[0]):\n",
        "        if y_train[idx] not in class_idx_dict_train.keys(): class_idx_dict_train[y_train[idx]] = list()\n",
        "        class_idx_dict_train[y_train[idx]].append(idx)\n",
        "\n",
        "    class_idx_dict_test = dict()\n",
        "    for idx in range(0, y_test.shape[0]):\n",
        "        if y_test[idx] not in class_idx_dict_test.keys(): class_idx_dict_test[y_test[idx]] = list()\n",
        "        class_idx_dict_test[y_test[idx]].append(idx)\n",
        "\n",
        "\n",
        "    # sampling indexes\n",
        "    class_idx_dict_train_sampled = dict()\n",
        "    for cls in class_idx_dict_train.keys():\n",
        "        cur_list = class_idx_dict_train[cls]\n",
        "        sampled_list = random.sample(cur_list, int(frac * len(cur_list)))\n",
        "        class_idx_dict_train_sampled[cls] = sampled_list\n",
        "\n",
        "\n",
        "    class_idx_dict_test_sampled = dict()\n",
        "    for cls in class_idx_dict_test.keys():\n",
        "        cur_list = class_idx_dict_test[cls]\n",
        "        sampled_list = random.sample(cur_list, int(frac * len(cur_list)))\n",
        "        class_idx_dict_test_sampled[cls] = sampled_list\n",
        "\n",
        "\n",
        "    # sampling instances by sampled indexes\n",
        "    x_train_sampled = list()\n",
        "    y_train_sampled = list()\n",
        "    x_test_sampled = list()\n",
        "    y_test_sampled = list()\n",
        "    train_sampled_idxes = list()\n",
        "    test_sampled_idxes = list()\n",
        "\n",
        "    for idxes_list in class_idx_dict_train_sampled.values():\n",
        "        train_sampled_idxes.extend(idxes_list)\n",
        "    for idxes_list in class_idx_dict_test_sampled.values():\n",
        "        test_sampled_idxes.extend(idxes_list)\n",
        "\n",
        "    random.shuffle(train_sampled_idxes)\n",
        "    random.shuffle(test_sampled_idxes)\n",
        "\n",
        "    for idx in train_sampled_idxes:\n",
        "        x_train_sampled.append(x_train[idx])\n",
        "        y_train_sampled.append(y_train[idx])\n",
        "    for idx in test_sampled_idxes:\n",
        "        x_test_sampled.append(x_test[idx])\n",
        "        y_test_sampled.append(y_test[idx])\n",
        "    \n",
        "    return np.array(x_train_sampled), np.array(y_train_sampled), np.array(x_test_sampled), np.array(y_test_sampled), train_sampled_idxes, test_sampled_idxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVjFeVChZnRC"
      },
      "outputs": [],
      "source": [
        "dataset = 'fashionMNIST'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pks3ppUAkAPK"
      },
      "source": [
        "**Full Size Dataset**\n",
        "\n",
        "10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yu06FcIl87A"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "datasize = 'full'\n",
        "classStr = ''\n",
        "\n",
        "\n",
        "x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx = sample_dataset(x_train, y_train, x_test, y_test, 1)\n",
        "# np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_full_idx)\n",
        "# np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_full_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9iqymckU1uq"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "datasize = 'full'\n",
        "classStr = ''\n",
        "\n",
        "\n",
        "x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx = sample_dataset(x_train, y_train, x_test, y_test, 1)\n",
        "np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_full_idx)\n",
        "np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_full_idx)\n",
        "\n",
        "\n",
        "model_full = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_full.summary())\n",
        "batch_size = 1\n",
        "epochs = 1   # 30\n",
        "model_full.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_full.fit(x_train_full, y_train_full, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "predict_res_full = model_full.predict(x_test_full)\n",
        "y_pred_full = np.argmax(predict_res_full,axis=1)\n",
        "accuracy_full = accuracy_score(y_test_full, y_pred_full)\n",
        "\n",
        "model_penultimate_full = tf.keras.Model(model_full.layers[0].input, model_full.layers[-2].output)  # model soft\n",
        "model_last_full = model_full.layers[-1]\n",
        "logits_train_full = model_penultimate_full(x_train_full)\n",
        "soft_pred_train_full = model_last_full(logits_train_full)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_full)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_full)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_full)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_full, y_train_full, x_test_full, y_test_full, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_'+ str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDyJHwbldacN"
      },
      "source": [
        "**Frac = 0.5**\n",
        "\n",
        "10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkSxUcmJs4zh"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "datasize = 'frac5'\n",
        "classStr = ''\n",
        "\n",
        "\n",
        "x_train_frac5, y_train_frac5, x_test_frac5, y_test_frac5, train_frac5_idx, test_frac5_idx = sample_dataset(x_train, y_train, x_test, y_test, 0.5)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_frac5_idx)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_frac5_idx)\n",
        "\n",
        "\n",
        "\n",
        "model_frac5 = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac5.summary())\n",
        "batch_size = 128\n",
        "epochs = 20   # 30\n",
        "model_frac5.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac5.fit(x_train_frac5, y_train_frac5, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "predict_res_frac5 = model_frac5.predict(x_test_frac5)\n",
        "y_pred_frac5 = np.argmax(predict_res_frac5,axis=1)\n",
        "accuracy_frac5 = accuracy_score(y_test_frac5, y_pred_frac5)\n",
        "\n",
        "\n",
        "model_penultimate_frac5 = tf.keras.Model(model_frac5.layers[0].input, model_frac5.layers[-2].output)\n",
        "model_last_frac5 = model_frac5.layers[-1]\n",
        "logits_train_frac5 = model_penultimate_frac5(x_train_frac5)\n",
        "soft_pred_train_frac5 = model_last_frac5(logits_train_frac5)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_frac5)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_frac5)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_frac5)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac5, y_train_frac5, x_test_frac5, y_test_frac5, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('content/drive/MyDrive/result_new/fashionMNIST_noisy_data/' + 'fashionMNIST' + '_' + str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMLbMGaaNOfp"
      },
      "source": [
        "**Frac = 0.5**\n",
        "\n",
        "5 classes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C00XNi2wcoDg",
        "outputId": "29e52d87-6075-40ea-e201-b13bdce3bc1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "input_shape = (28, 28, 1)\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCJReuhuVTbH",
        "outputId": "c11069e4-fc59-46f2-c33e-af395370878b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.65\n"
          ]
        }
      ],
      "source": [
        "classes = [0,1,2,3,4,5,6,7,8,9]\n",
        "label_map = dict()     # encode the labels\n",
        "for i in range(0, len(classes)):\n",
        "  label_map[classes[i]] = i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_true_idx =np.load( 'fashionMNIST_sampled_data/fashionMNIST_10cls_train_frac3_idx.npy')\n",
        "y_true = y_train[y_true_idx]\n",
        "y_noise = np.load('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_10cls' + '_frac3' + '_' + 'uniform' + '_65.npy')\n",
        "cnt = 0\n",
        "\n",
        "\n",
        "# encode the clean y_train label\n",
        "for i in range(0, y_true.shape[0]):\n",
        "  prev_label = y_true[i]\n",
        "  # print('prev_label:', prev_label)\n",
        "  y_true[i] = label_map[prev_label]\n",
        "  # print('cur_label:', label_map[i]])\n",
        "\n",
        "\n",
        "for i in range(0, len(y_true)):\n",
        "  if y_true[i] != y_noise[i]:\n",
        "    cnt += 1\n",
        "print(cnt/len(y_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arVUaDRneQBm"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "# classStr = '02468' # alternatives: '03568', '12479', '13478', '13579'\n",
        "# classes = [0,2,4,6,8] # alternatives: [0,3,5,6,8], [1,2,4,7,9], [1,3,4,7,8], [1,3,5,7,9]\n",
        "\n",
        "\n",
        "classes_list = [[0,2,4,6,8], [0,3,5,6,8], [1,2,4,7,9], [1,3,4,7,8], [1,3,5,7,9]]\n",
        "classStr_list = ['02468', '03568', '12479', '13478', '13579']\n",
        "\n",
        "\n",
        "for i in range(0, len(classes_list)):\n",
        "  classes = classes_list[i]\n",
        "  classStr = classStr_list[i]\n",
        "  \n",
        "  datasize_list = ['frac4', 'frac3']\n",
        "  class_frac_list = [4/5, 3/5]\n",
        "  for j in range(0, len(datasize_list)):\n",
        "    class_frac = class_frac_list[j]\n",
        "    datasize = datasize_list[j]\n",
        "                           \n",
        "    x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx = sample_dataset(x_train, y_train, x_test, y_test, class_frac)\n",
        "    x_train_frac5_5cls, y_train_frac5_5cls, x_test_frac5_5cls, y_test_frac5_5cls, train_frac5_5cls_idx, test_frac5_5cls_idx = sample_class(x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx, classes)\n",
        "    np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_frac5_5cls_idx)\n",
        "    np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_frac5_5cls_idx)\n",
        "\n",
        "    # encode the labels\n",
        "    label_map = dict()\n",
        "    for i in range(0, len(classes)):\n",
        "      label_map[classes[i]] = i\n",
        "    for i in range(0, y_train_frac5_5cls.shape[0]):\n",
        "      prev_label = y_train_frac5_5cls[i]\n",
        "      y_train_frac5_5cls[i] = label_map[prev_label]\n",
        "    for i in range(0, y_test_frac5_5cls.shape[0]):\n",
        "      prev_label = y_test_frac5_5cls[i]\n",
        "      y_test_frac5_5cls[i] = label_map[prev_label]\n",
        "\n",
        "\n",
        "\n",
        "    model_frac5_5cls = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=input_shape),\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(32),\n",
        "            layers.Dense(num_classes),\n",
        "            layers.Activation('softmax')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(model_frac5_5cls.summary())\n",
        "    batch_size = 128\n",
        "    epochs = 20   # 30\n",
        "    model_frac5_5cls.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    model_frac5_5cls.fit(x_train_frac5_5cls, y_train_frac5_5cls, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "\n",
        "    predict_res_frac5_5cls = model_frac5_5cls.predict(x_test_frac5_5cls)\n",
        "    y_pred_frac5_5cls = np.argmax(predict_res_frac5_5cls,axis=1)\n",
        "    accuracy_frac5_5cls = accuracy_score(y_test_frac5_5cls, y_pred_frac5_5cls)\n",
        "\n",
        "\n",
        "\n",
        "    model_penultimate_frac5_5cls = tf.keras.Model(model_frac5_5cls.layers[0].input, model_frac5_5cls.layers[-2].output)\n",
        "    model_last_frac5_5cls = model_frac5_5cls.layers[-1]\n",
        "    logits_train_frac5_5cls = model_penultimate_frac5_5cls(x_train_frac5_5cls)\n",
        "    soft_pred_train_frac5_5cls = model_last_frac5_5cls(logits_train_frac5_5cls)\n",
        "    np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_frac5_5cls)\n",
        "    np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_frac5_5cls)\n",
        "    np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_frac5_5cls)\n",
        "\n",
        "\n",
        "    for noise_type in noise_type_list:\n",
        "      for noise_ratio in noise_ratio_list:\n",
        "        y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac5_5cls, y_train_frac5_5cls, x_test_frac5_5cls, y_test_frac5_5cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "        np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_' + str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hle_ygppk7K8"
      },
      "source": [
        "**Frac = 0.2**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq0AkHylk_Sn"
      },
      "source": [
        "10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vcPatyyVGh6"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "datasize = 'frac2'\n",
        "classStr = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tugPzsOwk_E4"
      },
      "outputs": [],
      "source": [
        "x_train_frac2, y_train_frac2, x_test_frac2, y_test_frac2, train_frac2_idx, test_frac2_idx = sample_dataset(x_train, y_train, x_test, y_test, 0.2)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_10cls_train_'+ datasize +'_idx', train_frac2_idx)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_10cls_test_'+ datasize +'_idx', test_frac2_idx)\n",
        "\n",
        "\n",
        "model_frac2 = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac2.summary())\n",
        "batch_size = 56\n",
        "epochs = 20   # 30\n",
        "model_frac2.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac2.fit(x_train_frac2, y_train_frac2, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "\n",
        "predict_res_frac2 = model_frac2.predict(x_test_frac2)\n",
        "y_pred_frac2 = np.argmax(predict_res_frac2,axis=1)\n",
        "accuracy_frac2 = accuracy_score(y_test_frac2, y_pred_frac2)\n",
        "\n",
        "\n",
        "\n",
        "model_penultimate_frac2 = tf.keras.Model(model_frac2.layers[0].input, model_frac2.layers[-2].output)\n",
        "model_last_frac2 = model_frac2.layers[-1]\n",
        "logits_train_frac2 = model_penultimate_frac2(x_train_frac2)\n",
        "soft_pred_train_frac2 = model_last_frac2(logits_train_frac2)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_10cls_logits_train', logits_train_frac2)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_10cls_soft_pred_train', soft_pred_train_frac2)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_10cls_soft_pred_test', predict_res_frac2)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac2, y_train_frac2, x_test_frac2, y_test_frac2, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_' + str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn33MhmWlvEq"
      },
      "source": [
        "5 classes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6ZKsxnwMw2u"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "datasize = 'frac2'\n",
        "classStr = '02468' # alternatives: '03568', '12479', '13478', '13579'\n",
        "classes = [0,2,4,6,8] # alternatives: [0,3,5,6,8], [1,2,4,7,9], [1,3,4,7,8], [1,3,5,7,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOMftf6fk6Bg"
      },
      "outputs": [],
      "source": [
        "x_train_frac4, y_train_frac4, x_test_frac4, y_test_frac4, train_frac4_idx, test_frac4_idx = sample_dataset(x_train, y_train, x_test, y_test, 0.4)\n",
        "x_train_frac2_5cls, y_train_frac2_5cls, x_test_frac2_5cls, y_test_frac2_5cls, train_frac2_5cls_idx, test_frac2_5cls_idx = sample_class(x_train_frac4, y_train_frac4, x_test_frac4, y_test_frac4, train_frac4_idx, test_frac4_idx, classes)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize + '_idx', train_frac2_5cls_idx)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize + '_idx', test_frac2_5cls_idx)\n",
        "# encode the labels\n",
        "label_map = dict()\n",
        "for i in range(0, len(classes)):\n",
        "  label_map[classes[i]] = i\n",
        "for i in range(0, y_train_frac2_5cls.shape[0]):\n",
        "  prev_label = y_train_frac2_5cls[i]\n",
        "  y_train_frac2_5cls[i] = label_map[prev_label]\n",
        "for i in range(0, y_test_frac2_5cls.shape[0]):\n",
        "  prev_label = y_test_frac2_5cls[i]\n",
        "  y_test_frac2_5cls[i] = label_map[prev_label]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_frac2_5cls = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac2_5cls.summary())\n",
        "batch_size = 128\n",
        "epochs = 20   # 30\n",
        "model_frac2_5cls.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac2_5cls.fit(x_train_frac2_5cls, y_train_frac2_5cls, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "predict_res_frac2_5cls = model_frac2_5cls.predict(x_test_frac2_5cls)\n",
        "y_pred_frac2_5cls = np.argmax(predict_res_frac2_5cls,axis=1)\n",
        "accuracy_frac2_5cls = accuracy_score(y_test_frac2_5cls, y_pred_frac2_5cls)\n",
        "\n",
        "\n",
        "model_penultimate_frac2_5cls = tf.keras.Model(model_frac2_5cls.layers[0].input, model_frac2_5cls.layers[-2].output)\n",
        "model_last_frac2_5cls = model_frac2_5cls.layers[-1]\n",
        "logits_train_frac2_5cls = model_penultimate_frac2_5cls(x_train_frac2_5cls)\n",
        "soft_pred_train_frac2_5cls = model_last_frac2_5cls(logits_train_frac2_5cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_frac2_5cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_frac2_5cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_frac2_5cls)\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac2_5cls, y_train_frac2_5cls, x_test_frac2_5cls, y_test_frac2_5cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_' + str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml_g8hRxo5G5"
      },
      "source": [
        "2 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp6Ru2LmOK9-"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "datasize = 'frac2'\n",
        "classStr = '24' # alternatives: '26', '46', '06', '19', '47', '45', '07'\n",
        "classes = [2,4] # alternatives: [2,6], [4,6], [0,6], [1,9], [4,7], [4,5], [0,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGINDx2sluzy"
      },
      "outputs": [],
      "source": [
        "x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx = sample_dataset(x_train, y_train, x_test, y_test, 1)\n",
        "x_train_frac2_2cls, y_train_frac2_2cls, x_test_frac2_2cls, y_test_frac2_2cls, train_frac2_2cls_idx, test_frac2_2cls_idx = sample_class(x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx, classes)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize + '_idx', train_frac2_2cls_idx)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize + '_idx', test_frac2_2cls_idx)\n",
        "label_map = dict()\n",
        "for i in range(0, len(classes)):\n",
        "  label_map[classes[i]] = i\n",
        "for i in range(0, y_train_frac2_2cls.shape[0]):\n",
        "  prev_label = y_train_frac2_2cls[i]\n",
        "  y_train_frac2_2cls[i] = label_map[prev_label]\n",
        "for i in range(0, y_test_frac2_2cls.shape[0]):\n",
        "  prev_label = y_test_frac2_2cls[i]\n",
        "  y_test_frac2_2cls[i] = label_map[prev_label]\n",
        "\n",
        "\n",
        "\n",
        "model_frac2_2cls = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac2_2cls.summary())\n",
        "batch_size = 128\n",
        "epochs = 20   # 30\n",
        "model_frac2_2cls.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac2_2cls.fit(x_train_frac2_2cls, y_train_frac2_2cls, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "\n",
        "predict_res_frac2_2cls = model_frac2_2cls.predict(x_test_frac2_2cls)\n",
        "y_pred_frac2_2cls = np.argmax(predict_res_frac2_2cls,axis=1)\n",
        "accuracy_frac2_2cls = accuracy_score(y_test_frac2_2cls, y_pred_frac2_2cls)\n",
        "\n",
        "\n",
        "\n",
        "model_penultimate_frac2_2cls = tf.keras.Model(model_frac2_2cls.layers[0].input, model_frac2_2cls.layers[-2].output)\n",
        "model_last_frac2_2cls = model_frac2_2cls.layers[-1]\n",
        "logits_train_frac2_2cls = model_penultimate_frac2_2cls(x_train_frac2_2cls)\n",
        "soft_pred_train_frac2_2cls = model_last_frac2_2cls(logits_train_frac2_2cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_frac2_2cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_frac2_2cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_frac2_2cls)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac2_2cls, y_train_frac2_2cls, x_test_frac2_2cls, y_test_frac2_2cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_'+ str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utZz0u27ddDQ"
      },
      "source": [
        "**Frac = 0.1**\n",
        "\n",
        "10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPNN0IzFVO7D"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "datasize = 'frac1'\n",
        "classStr = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_IXHkw6pMun",
        "outputId": "56597e9b-c64d-438c-9b30-d68c787f6b7a"
      },
      "outputs": [],
      "source": [
        "x_train_frac1, y_train_frac1, x_test_frac1, y_test_frac1, train_frac1_idx, test_frac1_idx = sample_dataset(x_train, y_train, x_test, y_test, 0.1)\n",
        "np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr +'_train_'+ datasize + '_idx', train_frac1_idx)\n",
        "np.save('fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr +'_test_'+ datasize + '_idx', test_frac1_idx)\n",
        "\n",
        "\n",
        "\n",
        "model_frac1 = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac1.summary())\n",
        "batch_size = 56\n",
        "epochs = 20   # 30\n",
        "model_frac1.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac1.fit(x_train_frac1, y_train_frac1, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "\n",
        "predict_res_frac1 = model_frac1.predict(x_test_frac1)\n",
        "y_pred_frac1 = np.argmax(predict_res_frac1,axis=1)\n",
        "accuracy_frac1 = accuracy_score(y_test_frac1, y_pred_frac1)\n",
        "\n",
        "model_penultimate_frac1 = tf.keras.Model(model_frac1.layers[0].input, model_frac1.layers[-2].output)\n",
        "model_last_frac1 = model_frac1.layers[-1]\n",
        "logits_train_frac1 = model_penultimate_frac1(x_train_frac1)\n",
        "\n",
        "# logits_test_frac1 = model_penultimate_frac1(x_test_frac1)\n",
        "\n",
        "\n",
        "soft_pred_train_frac1 = model_last_frac1(logits_train_frac1)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr +'_logits_train', logits_train_frac1)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr +'_soft_pred_train', soft_pred_train_frac1)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr +'_soft_pred_test', predict_res_frac1)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac1, y_train_frac1, x_test_frac1, y_test_frac1, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_' + str(num_classes) + 'cls_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2Sd60rS0Ybv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IT36jdQyZT3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv7WNR5cQcAC"
      },
      "source": [
        "5 classes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kbhn6mKQu51"
      },
      "outputs": [],
      "source": [
        "num_classes = 5\n",
        "datasize = 'frac1'\n",
        "classStr = '02468' # alternatives: '03568', '12479', '13478', '13579'\n",
        "classes = [0,2,4,6,8] # alternatives: [0,3,5,6,8], [1,2,4,7,9], [1,3,4,7,8], [1,3,5,7,9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzKBsTyLpQX1"
      },
      "outputs": [],
      "source": [
        "x_train_frac2, y_train_frac2, x_test_frac2, y_test_frac2, train_frac2_idx, test_frac2_idx = sample_dataset(x_train, y_train, x_test, y_test, 0.2)\n",
        "x_train_frac1_5cls, y_train_frac1_5cls, x_test_frac1_5cls, y_test_frac1_5cls, train_frac1_5cls_idx, test_frac1_5cls_idx = sample_class(x_train_frac2, y_train_frac2, x_test_frac2, y_test_frac2, train_frac2_idx, test_frac2_idx, classes)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_frac1_5cls_idx)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_frac1_5cls_idx)\n",
        "# encode the labels\n",
        "label_map = dict()\n",
        "for i in range(0, len(classes)):\n",
        "  label_map[classes[i]] = i\n",
        "for i in range(0, y_train_frac1_5cls.shape[0]):\n",
        "  prev_label = y_train_frac1_5cls[i]\n",
        "  y_train_frac1_5cls[i] = label_map[prev_label]\n",
        "for i in range(0, y_test_frac1_5cls.shape[0]):\n",
        "  prev_label = y_test_frac1_5cls[i]\n",
        "  y_test_frac1_5cls[i] = label_map[prev_label]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_frac1_5cls = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac1_5cls.summary())\n",
        "batch_size = 128\n",
        "epochs = 20   # 30\n",
        "model_frac1_5cls.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac1_5cls.fit(x_train_frac1_5cls, y_train_frac1_5cls, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "predict_res_frac1_5cls = model_frac1_5cls.predict(x_test_frac1_5cls)\n",
        "y_pred_frac1_5cls = np.argmax(predict_res_frac1_5cls,axis=1)\n",
        "accuracy_frac1_5cls = accuracy_score(y_test_frac1_5cls, y_pred_frac1_5cls)\n",
        "\n",
        "\n",
        "model_penultimate_frac1_5cls = tf.keras.Model(model_frac1_5cls.layers[0].input, model_frac1_5cls.layers[-2].output)\n",
        "model_last_frac1_5cls = model_frac1_5cls.layers[-1]\n",
        "logits_train_frac1_5cls = model_penultimate_frac1_5cls(x_train_frac1_5cls)\n",
        "soft_pred_train_frac1_5cls = model_last_frac1_5cls(logits_train_frac1_5cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_frac1_5cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_frac1_5cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_frac1_5cls)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac1_5cls, y_train_frac1_5cls, x_test_frac1_5cls, y_test_frac1_5cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_' + str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "159HBxzfTMkz"
      },
      "source": [
        "2 classes  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6djLxV2Rx4z"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "datasize = 'frac1'\n",
        "classStr = '24' # alternatives: '26', '46', '06', '19', '47', '45', '07'\n",
        "classes = [2,4] # alternatives: [2,6], [4,6], [0,6], [1,9], [4,7], [4,5], [0,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJJ7rCvkpaPn"
      },
      "outputs": [],
      "source": [
        "x_train_frac5, y_train_frac5, x_test_frac5, y_test_frac5, train_frac5_idx, test_frac5_idx = sample_dataset(x_train, y_train, x_test, y_test, 0.5)\n",
        "x_train_frac1_2cls, y_train_frac1_2cls, x_test_frac1_2cls, y_test_frac1_2cls, train_frac1_2cls_idx, test_frac1_2cls_idx = sample_class(x_train_frac5, y_train_frac5, x_test_frac5, y_test_frac5, train_frac5_idx, test_frac5_idx, classes)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize + '_idx', train_frac1_2cls_idx)\n",
        "np.save('content/drive/MyDrive/result_new/fashionMNIST_sampled_data/fashionMNIST_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize + '_idx', test_frac1_2cls_idx)\n",
        "label_map = dict()\n",
        "for i in range(0, len(classes)):\n",
        "  label_map[classes[i]] = i\n",
        "for i in range(0, y_train_frac1_2cls.shape[0]):\n",
        "  prev_label = y_train_frac1_2cls[i]\n",
        "  y_train_frac1_2cls[i] = label_map[prev_label]\n",
        "for i in range(0, y_test_frac1_2cls.shape[0]):\n",
        "  prev_label = y_test_frac1_2cls[i]\n",
        "  y_test_frac1_2cls[i] = label_map[prev_label]\n",
        "\n",
        "\n",
        "\n",
        "model_frac1_2cls = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Activation('softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(model_frac1_2cls.summary())\n",
        "batch_size = 128\n",
        "epochs = 20   # 30\n",
        "model_frac1_2cls.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model_frac1_2cls.fit(x_train_frac1_2cls, y_train_frac1_2cls, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "\n",
        "predict_res_frac1_2cls = model_frac1_2cls.predict(x_test_frac1_2cls)\n",
        "y_pred_frac1_2cls = np.argmax(predict_res_frac1_2cls,axis=1)\n",
        "accuracy_frac1_2cls = accuracy_score(y_test_frac1_2cls, y_pred_frac1_2cls)\n",
        "\n",
        "\n",
        "model_penultimate_frac1_2cls = tf.keras.Model(model_frac1_2cls.layers[0].input, model_frac1_2cls.layers[-2].output)\n",
        "model_last_frac1_2cls = model_frac1_2cls.layers[-1]\n",
        "logits_train_frac1_2cls = model_penultimate_frac1_2cls(x_train_frac1_2cls)\n",
        "soft_pred_train_frac1_2cls = model_last_frac1_2cls(logits_train_frac1_2cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_frac1_2cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_frac1_2cls)\n",
        "np.save('fashionMNIST_logits_and_preds/fashionMNIST_'+ datasize + '_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_frac1_2cls)\n",
        "\n",
        "\n",
        "for noise_type in noise_type_list:\n",
        "  for noise_ratio in noise_ratio_list:\n",
        "    y_train_noisy, probs = get_noisy_labels('fashionMNIST', x_train_frac1_2cls, y_train_frac1_2cls, x_test_frac1_2cls, y_test_frac1_2cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n",
        "    np.save('fashionMNIST_noisy_data/' + 'fashionMNIST' + '_'+ str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ImAI6tLTM8x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
