{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9GxlYlGnB4j7"},"outputs":[],"source":["import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.image as mpimg\n","import io\n","import json\n","import numpy as np\n","import time\n","import datetime\n","import functools\n","import matplotlib.pyplot as plt\n","import sys\n","import random\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from noise_generate import get_noisy_labels\n","import pandas as pd\n","\n","import pandas as pd\n","import numpy as np\n","\n","#Data Visualization\n","import matplotlib.pyplot as plt\n","\n","#Text Color\n","from termcolor import colored\n","\n","#Train Test Split\n","from sklearn.model_selection import train_test_split\n","\n","#Model Evaluation\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n","from mlxtend.plotting import plot_confusion_matrix\n","\n","#Deep Learning\n","import tensorflow as tf\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional, Activation\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.utils import plot_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EujOudvZaVaz"},"outputs":[],"source":["def sample_class(x_train, y_train, x_test, y_test, train_idxes, test_idxes, classes):\n","    # sampling instances by sampled classes\n","    x_train_sampled_class = list()\n","    y_train_sampled_class = list()\n","    x_test_sampled_class = list()\n","    y_test_sampled_class = list()\n","    train_sampled_class_idxes = list()\n","    test_sampled_class_idxes = list()\n","\n","    # For training set\n","    for i in range(0, len(train_idxes)):\n","      if y_train[i] in classes:\n","        x_train_sampled_class.append(x_train[i])\n","        y_train_sampled_class.append(y_train[i])\n","        train_sampled_class_idxes.append(train_idxes[i])\n","    # For testing set\n","    for i in range(0, len(test_idxes)):\n","      if y_test[i] in classes:\n","        x_test_sampled_class.append(x_test[i])\n","        y_test_sampled_class.append(y_test[i])\n","        test_sampled_class_idxes.append(test_idxes[i])\n","\n","    return np.array(x_train_sampled_class), np.array(y_train_sampled_class), np.array(x_test_sampled_class), np.array(y_test_sampled_class), train_sampled_class_idxes, test_sampled_class_idxes\n","  \n","def sample_dataset_diff_frac(x_train, y_train, x_test, y_test, train_frac, test_frac):\n","    # get idxes list of each class in training set and testing set\n","    class_idx_dict_train = dict()\n","    for idx in range(0, y_train.shape[0]):\n","        if y_train[idx] not in class_idx_dict_train.keys(): class_idx_dict_train[y_train[idx]] = list()\n","        class_idx_dict_train[y_train[idx]].append(idx)\n","\n","    class_idx_dict_test = dict()\n","    for idx in range(0, y_test.shape[0]):\n","        if y_test[idx] not in class_idx_dict_test.keys(): class_idx_dict_test[y_test[idx]] = list()\n","        class_idx_dict_test[y_test[idx]].append(idx)\n","\n","\n","    # sampling indexes\n","    class_idx_dict_train_sampled = dict()\n","    for cls in class_idx_dict_train.keys():\n","        cur_list = class_idx_dict_train[cls]\n","        sampled_list = random.sample(cur_list, int(train_frac * len(cur_list)))\n","        class_idx_dict_train_sampled[cls] = sampled_list\n","\n","\n","    class_idx_dict_test_sampled = dict()\n","    for cls in class_idx_dict_test.keys():\n","        cur_list = class_idx_dict_test[cls]\n","        sampled_list = random.sample(cur_list, int(test_frac * len(cur_list)))\n","        class_idx_dict_test_sampled[cls] = sampled_list\n","\n","\n","    # sampling instances by sampled indexes\n","    x_train_sampled = list()\n","    y_train_sampled = list()\n","    x_test_sampled = list()\n","    y_test_sampled = list()\n","    train_sampled_idxes = list()\n","    test_sampled_idxes = list()\n","\n","    for idxes_list in class_idx_dict_train_sampled.values():\n","        train_sampled_idxes.extend(idxes_list)\n","    for idxes_list in class_idx_dict_test_sampled.values():\n","        test_sampled_idxes.extend(idxes_list)\n","\n","    random.shuffle(train_sampled_idxes)\n","    random.shuffle(test_sampled_idxes)\n","\n","    for idx in train_sampled_idxes:\n","        x_train_sampled.append(x_train[idx])\n","        y_train_sampled.append(y_train[idx])\n","    for idx in test_sampled_idxes:\n","        x_test_sampled.append(x_test[idx])\n","        y_test_sampled.append(y_test[idx])\n","    \n","    return np.array(x_train_sampled), np.array(y_train_sampled), np.array(x_test_sampled), np.array(y_test_sampled), train_sampled_idxes, test_sampled_idxes\n","\n","def sample_dataset(x_train, y_train, x_test, y_test, frac):\n","    # get idxes list of each class in training set and testing set\n","    class_idx_dict_train = dict()\n","    for idx in range(0, y_train.shape[0]):\n","        if y_train[idx] not in class_idx_dict_train.keys(): class_idx_dict_train[y_train[idx]] = list()\n","        class_idx_dict_train[y_train[idx]].append(idx)\n","\n","    class_idx_dict_test = dict()\n","    for idx in range(0, y_test.shape[0]):\n","        if y_test[idx] not in class_idx_dict_test.keys(): class_idx_dict_test[y_test[idx]] = list()\n","        class_idx_dict_test[y_test[idx]].append(idx)\n","\n","\n","    # sampling indexes\n","    class_idx_dict_train_sampled = dict()\n","    for cls in class_idx_dict_train.keys():\n","        cur_list = class_idx_dict_train[cls]\n","        sampled_list = random.sample(cur_list, int(frac * len(cur_list)))\n","        class_idx_dict_train_sampled[cls] = sampled_list\n","\n","\n","    class_idx_dict_test_sampled = dict()\n","    for cls in class_idx_dict_test.keys():\n","        cur_list = class_idx_dict_test[cls]\n","        sampled_list = random.sample(cur_list, int(frac * len(cur_list)))\n","        class_idx_dict_test_sampled[cls] = sampled_list\n","\n","\n","    # sampling instances by sampled indexes\n","    x_train_sampled = list()\n","    y_train_sampled = list()\n","    x_test_sampled = list()\n","    y_test_sampled = list()\n","    train_sampled_idxes = list()\n","    test_sampled_idxes = list()\n","\n","    for idxes_list in class_idx_dict_train_sampled.values():\n","        train_sampled_idxes.extend(idxes_list)\n","    for idxes_list in class_idx_dict_test_sampled.values():\n","        test_sampled_idxes.extend(idxes_list)\n","\n","    random.shuffle(train_sampled_idxes)\n","    random.shuffle(test_sampled_idxes)\n","\n","    for idx in train_sampled_idxes:\n","        x_train_sampled.append(x_train[idx])\n","        y_train_sampled.append(y_train[idx])\n","    for idx in test_sampled_idxes:\n","        x_test_sampled.append(x_test[idx])\n","        y_test_sampled.append(y_test[idx])\n","    \n","    return np.array(x_train_sampled), np.array(y_train_sampled), np.array(x_test_sampled), np.array(y_test_sampled), train_sampled_idxes, test_sampled_idxes\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lK_ecbg_RLq"},"outputs":[],"source":["dataset = 'agnews_10pct'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YMTBrOfbFkc"},"outputs":[],"source":["if not os.path.isdir('agnews_10pct_sampled_data'):\n","    os.mkdir('agnews_10pct_sampled_data')\n","if not os.path.isdir('agnews_10pct_logits_and_preds'):\n","    os.mkdir('agnews_10pct_logits_and_preds')\n","if not os.path.isdir('agnews_10pct_noisy_data'):\n","    os.mkdir('agnews_10pct_noisy_data')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxDI_mXsCGa_"},"outputs":[],"source":["#File Path\n","TRAIN_FILE_PATH = \"agnews_10pct_data/train_10pct_new.csv\"\n","TEST_FILE_PATH = \"agnews_10pct_data/test_10pct_new.csv\"\n","#Load Data\n","traindata = pd.read_csv(TRAIN_FILE_PATH, header=None)\n","testdata = pd.read_csv(TEST_FILE_PATH, header=None)\n","#Set Column Names \n","traindata.columns = ['ClassIndex', 'Title', 'Description']\n","testdata.columns = ['ClassIndex', 'Title', 'Description']#Combine Title and Description\n","x_train = traindata['Title'] + \" \" + traindata['Description'] # Combine title and description (better accuracy than using them as separate features)\n","y_train = traindata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n","x_test = testdata['Title'] + \" \" + testdata['Description'] # Combine title and description (better accuracy than using them as separate features)\n","y_test = testdata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaT2ToNBf-GT"},"outputs":[],"source":["noise_ratio_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.45, 0.5, 0.55, 0.6, 0.65]\n","noise_type_list = ['uniform', 'class-dependent', 'locally-concentrated']  # corresponding to three noise types: NCAR, NAR, NNAR"]},{"cell_type":"markdown","metadata":{"id":"Pks3ppUAkAPK"},"source":["**4 classes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9iqymckU1uq"},"outputs":[],"source":["\n","classes = [0,1,2,3]\n","classStr = ''  # if the dataset contains all classes instance, then no need for specifying classes in the dataset\n","\n","# datasize_list = ['full', 'frac5', 'frac2', 'frac1']\n","# class_frac_list = [1, 0.5, 0.2, 0.1]\n","datasize_list = ['frac9']\n","class_frac_list = [0.9]\n","\n","# datasize_list = ['frac9', 'frac8', 'frac7', 'frac6', 'frac4', 'frac3']\n","# class_frac_list = [0.9,0.8,0.7,0.6,0.4,0.3]\n","\n","\n","\n","num_classes = len(classes)  # has to be 3 in this loop\n","for j in range(0, len(datasize_list)):\n","  class_frac = class_frac_list[j]\n","  datasize = datasize_list[j]\n","  print(\"datasetsize: \", datasize)\n","  # datasize = 'full'  # 'frac5', 'frac2', 'frac1'\n","  # class_frac = 1  # 0.5, 0.2, 0.1\n","  # num_classes = len(classes)  # has to be 4 in this loop\n","\n","  x_train_full, y_train_full, x_test_full, y_test_full, train_full_idx, test_full_idx = sample_dataset(x_train, y_train, x_test, y_test, class_frac)\n","  np.save(dataset + '_sampled_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_full_idx)\n","  np.save(dataset + '_sampled_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_full_idx)\n","\n","\n","  #Max Length of sentences in Train Dataset\n","  maxLen = max([len(item.split()) for item in np.array(x_train_full)])\n","  print(\"maxLen for the subset is: \", maxLen)\n","\n","\n","  # Tokenize and Pad data\n","  vocab_size = 10000 # arbitrarily chosen\n","  embed_size = 32 # arbitrarily chosen\n","\n","  # Create and Fit tokenizer\n","  tok = Tokenizer(num_words=vocab_size)\n","  tok.fit_on_texts(x_train_full)\n","\n","  # Tokenize data\n","  x_train_full = tok.texts_to_sequences(x_train_full)\n","  x_test_full = tok.texts_to_sequences(x_test_full)\n","\n","  # Pad data\n","  x_train_full = pad_sequences(x_train_full, maxlen=maxLen)\n","  x_test_full = pad_sequences(x_test_full, maxlen=maxLen)\n","\n","\n","  model_full = keras.Sequential(\n","      [\n","      Embedding(vocab_size, embed_size, input_length=maxLen),\n","      Bidirectional(LSTM(128, return_sequences=True)),\n","      Bidirectional(LSTM(64, return_sequences=True)),\n","      GlobalMaxPooling1D(),\n","      #  Dense(1024),\n","      #  Dropout(0.25),\n","      #  Dense(512),\n","      #  Dropout(0.25),\n","      Dense(256),\n","      Dropout(0.25),\n","      Dense(128),\n","      Dropout(0.25),\n","      Dense(64),\n","      Dropout(0.25),\n","      Dense(4),\n","      Activation('softmax')\n","      ]\n","  )\n","  model_full.summary()\n","\n","\n","\n","  callbacks = [\n","      EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n","          monitor='val_accuracy',\n","          min_delta=1e-4,\n","          patience=6,\n","          verbose=1\n","      ),\n","      ModelCheckpoint(\n","          filepath='weights.h5',\n","          monitor='val_accuracy', \n","          mode='max', \n","          save_best_only=True,\n","          save_weights_only=True,\n","          verbose=1\n","      )\n","  ]\n","\n","  batch_size=256  #256\n","  epochs=20   #20\n","  model_full.compile(loss='sparse_categorical_crossentropy', #Sparse Categorical Crossentropy Loss because data is not one-hot encoded\n","                optimizer='adam', \n","                metrics=['accuracy']) \n","\n","  model_full.fit(x_train_full, \n","            y_train_full, \n","            batch_size=batch_size, \n","            validation_data=(x_test_full, y_test_full), \n","            epochs=epochs, \n","            callbacks=callbacks)\n","\n","\n","\n","  predict_res_full = model_full.predict(x_test_full)\n","  y_pred_full = np.argmax(predict_res_full,axis=1)\n","  accuracy_full = accuracy_score(y_test_full, y_pred_full)\n","\n","  model_penultimate_full = tf.keras.Model(model_full.layers[0].input, model_full.layers[-2].output)  # model soft\n","  model_last_full = model_full.layers[-1]\n","  logits_train_full = model_penultimate_full(x_train_full)\n","  soft_pred_train_full = model_last_full(logits_train_full)\n","  np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_full)\n","  np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_full)\n","  np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_full)\n","\n","\n","  for noise_type in noise_type_list:\n","    for noise_ratio in noise_ratio_list:\n","      y_train_noisy, probs = get_noisy_labels(dataset, x_train_full, y_train_full, x_test_full, y_test_full, num_classes, datasize, noise_type, noise_ratio, classStr)\n","      np.save(dataset + '_noisy_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"]},{"cell_type":"markdown","metadata":{"id":"A1772TSXP4a-"},"source":["**3 Classes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd3BQlD3fjtc"},"outputs":[],"source":["#File Path\n","TRAIN_FILE_PATH = \"agnews_10pct_data/train_10pct_new.csv\"\n","TEST_FILE_PATH = \"agnews_10pct_data/test_10pct_new.csv\"\n","#Load Data\n","traindata = pd.read_csv(TRAIN_FILE_PATH, header=None)\n","testdata = pd.read_csv(TEST_FILE_PATH, header=None)\n","#Set Column Names \n","traindata.columns = ['ClassIndex', 'Title', 'Description']\n","testdata.columns = ['ClassIndex', 'Title', 'Description']#Combine Title and Description\n","x_train = traindata['Title'] + \" \" + traindata['Description'] # Combine title and description (better accuracy than using them as separate features)\n","y_train = traindata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n","x_test = testdata['Title'] + \" \" + testdata['Description'] # Combine title and description (better accuracy than using them as separate features)\n","y_test = testdata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCgHzR3C6nlk"},"outputs":[],"source":["\n","classes_list = [[0,1,2], [1,2,3], [0,1,3], [0,2,3]]\n","classStr_list = ['012', '123', '013', '023']\n","\n","\n","\n","for i in range(0, len(classes_list)):\n","  classes = classes_list[i]\n","  classStr = classStr_list[i]\n","\n","\n","  # print(\"CURRENT CLASSSTR ===================================: \", classStr)\n","  # datasize_list = ['frac5', 'frac2', 'frac1']\n","  # class_frac_list = [10/15, 4/15, 2/15]\n","\n","  datasize_list = ['frac7', 'frac6', 'frac4', 'frac3']\n","  class_frac_list = [14/15, 12/15, 8/15, 6/15]\n","\n","  num_classes = len(classes)  # has to be 3 in this loop\n","  for j in range(0, len(datasize_list)):\n","    class_frac = class_frac_list[j]\n","    datasize = datasize_list[j]\n","\n","    x_train_classFrac, y_train_classFrac, x_test_classFrac, y_test_classFrac, train_classFrac_idx, test_classFrac_idx = sample_dataset(x_train, y_train, x_test, y_test, class_frac)\n","    x_train_classFrac_3cls, y_train_classFrac_3cls, x_test_classFrac_3cls, y_test_classFrac_3cls, train_classFrac_3cls_idx, test_classFrac_3cls_idx = sample_class(x_train_classFrac, y_train_classFrac, x_test_classFrac, y_test_classFrac, train_classFrac_idx, test_classFrac_idx, classes)\n","    \n","    # print(\"CURRENT class_Frac ===================================: \", class_frac)\n","    # print(\"CURRENT dataset size ===================================: \", datasize)\n","    # print(\"CURRENT class_distribution ===================================: \", np.unique(y_train_classFrac_3cls, return_counts=True))\n","\n","    \n","    \n","    np.save(dataset + '_sampled_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_classFrac_3cls_idx)\n","    np.save(dataset + '_sampled_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_classFrac_3cls_idx)\n","\n","      # encode the labels\n","    label_map = dict()\n","    for i in range(0, len(classes)):\n","      label_map[classes[i]] = i\n","    for i in range(0, y_train_classFrac_3cls.shape[0]):\n","      prev_label = y_train_classFrac_3cls[i]\n","      y_train_classFrac_3cls[i] = label_map[prev_label]\n","    for i in range(0, y_test_classFrac_3cls.shape[0]):\n","      prev_label = y_test_classFrac_3cls[i]\n","      y_test_classFrac_3cls[i] = label_map[prev_label]\n","\n","\n","\n","    #Max Length of sentences in Train Dataset\n","    maxLen = max([len(item.split()) for item in np.array(x_train_classFrac_3cls)])\n","    # print(\"maxLen for the subset is: \", maxLen)\n","\n","\n","    # Tokenize and Pad data\n","    vocab_size = 10000 # arbitrarily chosen\n","    embed_size = 32 # arbitrarily chosen\n","\n","    # Create and Fit tokenizer\n","    tok = Tokenizer(num_words=vocab_size)\n","    tok.fit_on_texts(x_train_classFrac_3cls)\n","\n","    # Tokenize data\n","    x_train_classFrac_3cls = tok.texts_to_sequences(x_train_classFrac_3cls)\n","    x_test_classFrac_3cls = tok.texts_to_sequences(x_test_classFrac_3cls)\n","\n","    # Pad data\n","    x_train_classFrac_3cls = pad_sequences(x_train_classFrac_3cls, maxlen=maxLen)\n","    x_test_classFrac_3cls = pad_sequences(x_test_classFrac_3cls, maxlen=maxLen)\n","\n","\n","    model_classFrac_3cls = keras.Sequential(\n","        [\n","        Embedding(vocab_size, embed_size, input_length=maxLen),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Bidirectional(LSTM(64, return_sequences=True)),\n","        GlobalMaxPooling1D(),\n","        #  Dense(1024),\n","        #  Dropout(0.25),\n","        #  Dense(512),\n","        #  Dropout(0.25),\n","        Dense(256),\n","        Dropout(0.25),\n","        Dense(128),\n","        Dropout(0.25),\n","        Dense(64),\n","        Dropout(0.25),\n","        Dense(4),\n","        Activation('softmax')\n","        ]\n","    )\n","    # model_classFrac_3cls.summary()\n","\n","\n","\n","    callbacks = [\n","        EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n","            monitor='val_accuracy',\n","            min_delta=1e-4,\n","            patience=6,\n","            verbose=1\n","        ),\n","        ModelCheckpoint(\n","            filepath='weights.h5',\n","            monitor='val_accuracy', \n","            mode='max', \n","            save_best_only=True,\n","            save_weights_only=True,\n","            verbose=1\n","        )\n","    ]\n","\n","    batch_size=256  #256\n","    epochs=20   #20\n","    model_classFrac_3cls.compile(loss='sparse_categorical_crossentropy', #Sparse Categorical Crossentropy Loss because data is not one-hot encoded\n","                  optimizer='adam', \n","                  metrics=['accuracy']) \n","\n","    model_classFrac_3cls.fit(x_train_classFrac_3cls, \n","              y_train_classFrac_3cls, \n","              batch_size=batch_size, \n","              validation_data=(x_test_classFrac_3cls, y_test_classFrac_3cls), \n","              epochs=epochs, \n","              callbacks=callbacks)\n","\n","\n","\n","    predict_res_classFrac_3cls = model_classFrac_3cls.predict(x_test_classFrac_3cls)\n","    y_pred_classFrac_3cls = np.argmax(predict_res_classFrac_3cls,axis=1)\n","    accuracy_classFrac_3cls = accuracy_score(y_test_classFrac_3cls, y_pred_classFrac_3cls)\n","\n","    model_penultimate_classFrac_3cls = tf.keras.Model(model_classFrac_3cls.layers[0].input, model_classFrac_3cls.layers[-2].output)  # model soft\n","    model_last_classFrac_3cls = model_classFrac_3cls.layers[-1]\n","    logits_train_classFrac_3cls = model_penultimate_classFrac_3cls(x_train_classFrac_3cls)\n","    soft_pred_train_classFrac_3cls = model_last_classFrac_3cls(logits_train_classFrac_3cls)\n","    np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_classFrac_3cls)\n","    np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_classFrac_3cls)\n","    np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_classFrac_3cls)\n","\n","    for noise_type in noise_type_list:\n","      for noise_ratio in noise_ratio_list:\n","        y_train_noisy, probs = get_noisy_labels(dataset, x_train_classFrac_3cls, y_train_classFrac_3cls, x_test_classFrac_3cls, y_test_classFrac_3cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n","        np.save(dataset + '_noisy_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_zHmGWeb_rA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"pjpeoRzVcBoz"},"source":["**2 classes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1nD__wQfupd"},"outputs":[],"source":["#File Path\n","TRAIN_FILE_PATH = \"agnews_10pct_data/train_10pct_new.csv\"\n","TEST_FILE_PATH = \"agnews_10pct_data/test_10pct_new.csv\"\n","#Load Data\n","traindata = pd.read_csv(TRAIN_FILE_PATH, header=None)\n","testdata = pd.read_csv(TEST_FILE_PATH, header=None)\n","#Set Column Names \n","traindata.columns = ['ClassIndex', 'Title', 'Description']\n","testdata.columns = ['ClassIndex', 'Title', 'Description']#Combine Title and Description\n","x_train = traindata['Title'] + \" \" + traindata['Description'] # Combine title and description (better accuracy than using them as separate features)\n","y_train = traindata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n","x_test = testdata['Title'] + \" \" + testdata['Description'] # Combine title and description (better accuracy than using them as separate features)\n","y_test = testdata['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDKg8xf1b_nu"},"outputs":[],"source":["\n","classes_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n","classStr_list = ['01', '02', '03', '12', '13', '23']\n","\n","\n","\n","for i in range(0, len(classes_list)):\n","  classes = classes_list[i]\n","  classStr = classStr_list[i]\n","\n","\n","  # print(\"CURRENT CLASSSTR ===================================: \", classStr)\n","  # datasize_list = ['frac5', 'frac2', 'frac1']\n","  # class_frac_list = [1, 2/5, 1/5]\n","  datasize_list = ['frac4', 'frac3']\n","  class_frac_list = [4/5, 3/5]\n","\n","  num_classes = len(classes)  # has to be 3 in this loop\n","  for j in range(0, len(datasize_list)):\n","    class_frac = class_frac_list[j]\n","    datasize = datasize_list[j]\n","\n","    x_train_classFrac, y_train_classFrac, x_test_classFrac, y_test_classFrac, train_classFrac_idx, test_classFrac_idx = sample_dataset(x_train, y_train, x_test, y_test, class_frac)\n","    x_train_classFrac_2cls, y_train_classFrac_2cls, x_test_classFrac_2cls, y_test_classFrac_2cls, train_classFrac_2cls_idx, test_classFrac_2cls_idx = sample_class(x_train_classFrac, y_train_classFrac, x_test_classFrac, y_test_classFrac, train_classFrac_idx, test_classFrac_idx, classes)\n","    \n","    # print(\"CURRENT class_Frac ===================================: \", class_frac)\n","    # print(\"CURRENT dataset size ===================================: \", datasize)\n","    # print(\"CURRENT class_distribution ===================================: \", np.unique(y_train_classFrac_2cls, return_counts=True))\n","\n","    \n","    \n","    np.save(dataset + '_sampled_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_train_'+ datasize +'_idx', train_classFrac_2cls_idx)\n","    np.save(dataset + '_sampled_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_test_'+ datasize +'_idx', test_classFrac_2cls_idx)\n","\n","      # encode the labels\n","    label_map = dict()\n","    for i in range(0, len(classes)):\n","      label_map[classes[i]] = i\n","    for i in range(0, y_train_classFrac_2cls.shape[0]):\n","      prev_label = y_train_classFrac_2cls[i]\n","      y_train_classFrac_2cls[i] = label_map[prev_label]\n","    for i in range(0, y_test_classFrac_2cls.shape[0]):\n","      prev_label = y_test_classFrac_2cls[i]\n","      y_test_classFrac_2cls[i] = label_map[prev_label]\n","\n","\n","\n","    #Max Length of sentences in Train Dataset\n","    maxLen = max([len(item.split()) for item in np.array(x_train_classFrac_2cls)])\n","    # print(\"maxLen for the subset is: \", maxLen)\n","\n","\n","    # Tokenize and Pad data\n","    vocab_size = 10000 # arbitrarily chosen\n","    embed_size = 32 # arbitrarily chosen\n","\n","    # Create and Fit tokenizer\n","    tok = Tokenizer(num_words=vocab_size)\n","    tok.fit_on_texts(x_train_classFrac_2cls)\n","\n","    # Tokenize data\n","    x_train_classFrac_2cls = tok.texts_to_sequences(x_train_classFrac_2cls)\n","    x_test_classFrac_2cls = tok.texts_to_sequences(x_test_classFrac_2cls)\n","\n","    # Pad data\n","    x_train_classFrac_2cls = pad_sequences(x_train_classFrac_2cls, maxlen=maxLen)\n","    x_test_classFrac_2cls = pad_sequences(x_test_classFrac_2cls, maxlen=maxLen)\n","\n","\n","    model_classFrac_2cls = keras.Sequential(\n","        [\n","        Embedding(vocab_size, embed_size, input_length=maxLen),\n","        Bidirectional(LSTM(128, return_sequences=True)),\n","        Bidirectional(LSTM(64, return_sequences=True)),\n","        GlobalMaxPooling1D(),\n","        #  Dense(1024),\n","        #  Dropout(0.25),\n","        #  Dense(512),\n","        #  Dropout(0.25),\n","        Dense(256),\n","        Dropout(0.25),\n","        Dense(128),\n","        Dropout(0.25),\n","        Dense(64),\n","        Dropout(0.25),\n","        Dense(4),\n","        Activation('softmax')\n","        ]\n","    )\n","    # model_classFrac_2cls.summary()\n","\n","\n","\n","    callbacks = [\n","        EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n","            monitor='val_accuracy',\n","            min_delta=1e-4,\n","            patience=6,\n","            verbose=1\n","        ),\n","        ModelCheckpoint(\n","            filepath='weights.h5',\n","            monitor='val_accuracy', \n","            mode='max', \n","            save_best_only=True,\n","            save_weights_only=True,\n","            verbose=1\n","        )\n","    ]\n","\n","    batch_size=256  #256\n","    epochs=20   #20\n","    model_classFrac_2cls.compile(loss='sparse_categorical_crossentropy', #Sparse Categorical Crossentropy Loss because data is not one-hot encoded\n","                  optimizer='adam', \n","                  metrics=['accuracy']) \n","\n","    model_classFrac_2cls.fit(x_train_classFrac_2cls, \n","              y_train_classFrac_2cls, \n","              batch_size=batch_size, \n","              validation_data=(x_test_classFrac_2cls, y_test_classFrac_2cls), \n","              epochs=epochs, \n","              callbacks=callbacks)\n","\n","\n","\n","    predict_res_classFrac_2cls = model_classFrac_2cls.predict(x_test_classFrac_2cls)\n","    y_pred_classFrac_2cls = np.argmax(predict_res_classFrac_2cls,axis=1)\n","    accuracy_classFrac_2cls = accuracy_score(y_test_classFrac_2cls, y_pred_classFrac_2cls)\n","\n","    model_penultimate_classFrac_2cls = tf.keras.Model(model_classFrac_2cls.layers[0].input, model_classFrac_2cls.layers[-2].output)  # model soft\n","    model_last_classFrac_2cls = model_classFrac_2cls.layers[-1]\n","    logits_train_classFrac_2cls = model_penultimate_classFrac_2cls(x_train_classFrac_2cls)\n","    soft_pred_train_classFrac_2cls = model_last_classFrac_2cls(logits_train_classFrac_2cls)\n","    np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_logits_train', logits_train_classFrac_2cls)\n","    np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_train', soft_pred_train_classFrac_2cls)\n","    np.save(dataset + '_logits_and_preds/' + dataset + '_'+ datasize +'_'+ str(num_classes) + 'cls' + classStr + '_soft_pred_test', predict_res_classFrac_2cls)\n","\n","    for noise_type in noise_type_list:\n","      for noise_ratio in noise_ratio_list:\n","        y_train_noisy, probs = get_noisy_labels(dataset, x_train_classFrac_2cls, y_train_classFrac_2cls, x_test_classFrac_2cls, y_test_classFrac_2cls, num_classes, datasize, noise_type, noise_ratio, classStr)\n","        np.save(dataset + '_noisy_data/' + dataset + '_'+ str(num_classes) + 'cls' + classStr + '_' + datasize + '_' + noise_type + '_' + str(int(noise_ratio*100)), y_train_noisy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VLYRj2oVJIl"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}